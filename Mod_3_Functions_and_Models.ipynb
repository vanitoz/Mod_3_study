{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mod 3 Functions and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formal notation of logistic regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Describe the need for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "It allows us to understand the probabilities within \n",
    "multiple classes where a linear regression wouldn't appropriately fit. \n",
    "\n",
    "It is a classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Interpret the parameters of a logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "??? \n",
    "\n",
    "What is the purpose of reshape(-1, 1)  Looks like we are scaling -1 to 1 \n",
    "\n",
    "Like if we erase it it does the same thing\n",
    "\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "???\n",
    "\n",
    "When doing the logit model, it includes male but not female, why or what is the assumption?\n",
    "\n",
    "???\n",
    "\n",
    "ANSWERED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "???\n",
    "\n",
    "What do the array of the coef mean at the end of the model?\n",
    "\n",
    "Is it the prob or the odds that X is 1 or close to 1\n",
    "\n",
    "???\n",
    "\n",
    "ANSWERED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Starting to build a logisitic regression we want to create binary datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_bin = income > 4\n",
    "print(income_bin)\n",
    "income_bin = income_bin.astype(int)  \n",
    "print(income_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is how to plot it\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "fig.suptitle('age vs binary income', fontsize=16)\n",
    "plt.scatter(age, income_bin)\n",
    "plt.xlabel('age', fontsize=14)\n",
    "plt.ylabel('monthly income (> or < 4000)', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphing the Train and Test data to find the optimal alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_all, X_test_all, y_train, y_test = preprocess(X, y)\n",
    "\n",
    "train_mse = []\n",
    "test_mse = []\n",
    "alphas = []\n",
    "\n",
    "for alpha in np.linspace(0, 200, num=50):\n",
    "    lasso = Lasso(alpha=alpha)\n",
    "    lasso.fit(X_train_all, y_train)\n",
    "    \n",
    "    train_preds = lasso.predict(X_train_all)\n",
    "    train_mse.append(mean_squared_error(y_train, train_preds))\n",
    "    \n",
    "    test_preds = lasso.predict(X_test_all)\n",
    "    test_mse.append(mean_squared_error(y_test, test_preds))\n",
    "    \n",
    "    alphas.append(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(alphas, train_mse, label='Train')\n",
    "ax.plot(alphas, test_mse, label='Test')\n",
    "ax.set_xlabel('Alpha')\n",
    "ax.set_ylabel('MSE')\n",
    "\n",
    "# np.argmin() returns the index of the minimum value in a list\n",
    "optimal_alpha = alphas[np.argmin(test_mse)]\n",
    "\n",
    "# Add a vertical line where the test MSE is minimized\n",
    "ax.axvline(optimal_alpha, color='black', linestyle='--')\n",
    "ax.legend();\n",
    "\n",
    "print(f'Optimal Alpha Value: {int(optimal_alpha)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 of Log Reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-54c9915f2739>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Do a Train Test Split on your Dataframe, Target\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Instantiate a Logistic Regression Model (liblinear is recommende for small datasets)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mlogreg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'liblinear'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "# Do a Train Test Split on your Dataframe, Target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "# Instantiate a Logistic Regression Model (liblinear is recommende for small datasets)\n",
    "logreg = LogisticRegression(C=1e5, solver='liblinear')\n",
    "\n",
    "# fit the model \n",
    "logreg_model = logreg.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 of Log Reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the coefficients\n",
    "coef = logreg_model.coef_\n",
    "interc = logreg_model.intercept_\n",
    "\n",
    "# Create the linear predictor\n",
    "line_pred = (interc + coef * X )  # Y = (B0 + B1*X1)\n",
    "def what X is above\n",
    "\n",
    "# We want to calc the probability that this instance X is at class 1\n",
    "\n",
    "P(1 | x) = np.exp(line_pred) / (1 + np.exp(line_pred)  #\n",
    "                                \n",
    "P(1 | x) = ((np.e)**(-0.62))/(1+(np.e)**(-0.62))\n",
    "\n",
    "# Perform the log transformation (This is generating the Odds)\n",
    "Odds = P(x) / (1 - P(x))\n",
    "\n",
    "# Generating the log of the odds values, \n",
    "LoggedOdds = np.log(Odds)  # reminder that this is = (B0 + B1*X1)\n",
    "                                                                \n",
    "# Sort the numbers to make sure plot looks right\n",
    "age_ordered, mod_income_ordered = zip(*sorted(zip(age ,mod_income.ravel()),key=lambda x: x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy of the log reg of the dataset on correct values\n",
    "logreg.score()\n",
    "\n",
    "#Finding Baseline\n",
    "\n",
    "baseline = y_test.value_counts()[0] / y_test.count()\n",
    "\n",
    "baseline\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the Logistic Regression for this equation\n",
    "\n",
    "ð‘Œ = âˆ’4.64 + 0.077*ð‘‹1 + 0.045*ð‘‹2\n",
    "\n",
    "X1 = 45\n",
    "\n",
    "X2 = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will compute the logged odds of the equation above\n",
    "BMI_45_age_25_logodds = -4.64 + 45*0.077 + 0.045*25\n",
    "BMI_45_age_25_logodds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute theh odds from the log odds by exponentiating it\n",
    "# convert log-odds to odds\n",
    "\n",
    "BMI_45_age_25_odds = np.exp(BMI_45_age_25_logodds)\n",
    "BMI_45_age_25_odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the probability from the odds\n",
    "\n",
    "BMI_45_age_25_prob = (BMI_45_age_25_odds)/(1+BMI_45_age_25_odds)\n",
    "BMI_45_age_25_prob\n",
    "\n",
    "# The odds that a person who is 25 yr and bmi of 45 has diabetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Be sure to create bar chart for the outcome for the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "*Always give a bar chart of the distribution of the outcome, gives context to what your model will look like*\n",
    "*Visualization that X variable has on the proportion of a class (target), allows to check on it, with a stacked bar chart*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Log Reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 6))\n",
    "fig.suptitle('logistic regression', fontsize=16)\n",
    "plt.scatter(age, income_bin)\n",
    "plt.xlabel('age', fontsize=14)\n",
    "plt.ylabel('monthly income', fontsize=14)\n",
    "plt.plot(age_ordered, mod_income_ordered, c='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values\n",
    "X = X.fillna(value=0) \n",
    "for col in X.columns:\n",
    "    # Subtract the minimum and divide by the range forcing a scale of 0 to 1 for each feature\n",
    "    X[col] = (X[col] - min(X[col]))/ (max(X[col]) - min(X[col])) \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to calculate the precision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y, y_hat):\n",
    "    # Could also use confusion matrix\n",
    "    y_y_hat = list(zip(y, y_hat))\n",
    "    tp = sum([1 for i in y_y_hat if i[0] == 1 and i[1] == 1])\n",
    "    fp = sum([1 for i in y_y_hat if i[0] == 0 and i[1] == 1])\n",
    "    return tp / float(tp + fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to calculate the recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y, y_hat):\n",
    "    # Could also use confusion matrix\n",
    "    y_y_hat = list(zip(y, y_hat))\n",
    "    tp = sum([1 for i in y_y_hat if i[0] == 1 and i[1] == 1])\n",
    "    fn = sum([1 for i in y_y_hat if i[0] == 1 and i[1] == 0])\n",
    "    return tp / float(tp + fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to calculate the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y, y_hat):\n",
    "    # Could also use confusion matrix\n",
    "    y_y_hat = list(zip(y, y_hat))\n",
    "    tp = sum([1 for i in y_y_hat if i[0] == 1 and i[1] == 1])\n",
    "    tn = sum([1 for i in y_y_hat if i[0] == 0 and i[1] == 0])\n",
    "    return (tp + tn) / float(len(y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to calculate the F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y, y_hat):\n",
    "    precision_score = precision(y, y_hat)\n",
    "    recall_score = recall(y, y_hat)\n",
    "    numerator = precision_score * recall_score\n",
    "    denominator = precision_score + recall_score\n",
    "    return 2 * (numerator / denominator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps to Import Log Reg, Instantiate Log Reg, and Fit to training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Instantiate LogisticRegression \n",
    "logreg = LogisticRegression(fit_intercept=False, C=1e12, solver='liblinear')\n",
    "\n",
    "# Fit to training data\n",
    "model_log = logreg.fit(X_train, y_train)\n",
    "model_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Calculate the precision, recall, accuracy, and F1 score of your classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_train = logreg.predict(X_train)\n",
    "y_hat_test = logreg.predict(X_test)\n",
    "\n",
    "print('Training Precision: ', precision(y_train, y_hat_train))\n",
    "print('Testing Precision: ', precision(y_test, y_hat_test))\n",
    "print('\\n\\n')\n",
    "\n",
    "print('Training Recall: ', recall(y_train, y_hat_train))\n",
    "print('Testing Recall: ', recall(y_test, y_hat_test))\n",
    "print('\\n\\n')\n",
    "\n",
    "print('Training Accuracy: ', accuracy(y_train, y_hat_train))\n",
    "print('Testing Accuracy: ', accuracy(y_test, y_hat_test))\n",
    "print('\\n\\n')\n",
    "\n",
    "print('Training F1-Score: ', f1(y_train, y_hat_train))\n",
    "print('Testing F1-Score: ', f1(y_test, y_hat_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and running the results of your classification data using SKLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "\n",
    "print('Training Precision: ', precision_score(y_train, y_hat_train))\n",
    "print('Testing Precision: ', precision_score(y_test, y_hat_test))\n",
    "print('\\n\\n')\n",
    "\n",
    "print('Training Recall: ', recall_score(y_train, y_hat_train))\n",
    "print('Testing Recall: ', recall_score(y_test, y_hat_test))\n",
    "print('\\n\\n')\n",
    "\n",
    "print('Training Accuracy: ', accuracy_score(y_train, y_hat_train))\n",
    "print('Testing Accuracy: ', accuracy_score(y_test, y_hat_test))\n",
    "print('\\n\\n')\n",
    "\n",
    "print('Training F1-Score: ', f1_score(y_train, y_hat_train))\n",
    "print('Testing F1-Score: ', f1_score(y_test, y_hat_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Showing our Confustion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test,y_pred_class)\n",
    "classes = ['Perished', 'Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion Matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save confusion matrix and slice into four pieces\n",
    "confusion = metrics.confusion_matrix(y_test, y_pred_class)\n",
    "TP = confusion[1, 1]\n",
    "TN = confusion[0, 0]\n",
    "FP = confusion[0, 1]\n",
    "FN = confusion[1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Showing off the ROC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: first argument is true values, second argument is predicted probabilities\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_prob)\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.title('ROC curve for diabetes classifier')\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function that does TTS, Scaling, and coding with dummies all at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X, y):\n",
    "    '''Takes in features and target and implements all preprocessing steps for categorical and continuous features returning \n",
    "    train and test DataFrames with targets'''\n",
    "    # Train-test split (75-25), set seed to 10\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=10)\n",
    "    # Remove \"object\"-type features and SalesPrice from X\n",
    "    cont_features = [col for col in X.columns if X[col].dtype in [np.float64, np.int64]]\n",
    "    X_train_cont = X_train.loc[:, cont_features]\n",
    "    X_test_cont = X_test.loc[:, cont_features]\n",
    "    # Impute missing values with median using SimpleImputer\n",
    "    impute = SimpleImputer(strategy='median')\n",
    "    X_train_imputed = impute.fit_transform(X_train_cont)\n",
    "    X_test_imputed = impute.transform(X_test_cont)\n",
    "    # Scale the train and test data\n",
    "    ss = StandardScaler()\n",
    "    X_train_imputed_scaled = ss.fit_transform(X_train_imputed)\n",
    "    X_test_imputed_scaled = ss.transform(X_test_imputed)\n",
    "    # Create X_cat which contains only the categorical variables\n",
    "    features_cat = [col for col in X.columns if X[col].dtype in [np.object]]\n",
    "    X_train_cat = X_train.loc[:, features_cat]\n",
    "    X_test_cat = X_test.loc[:, features_cat]\n",
    "    # Fill nans with a value indicating that that it is missing\n",
    "    X_train_cat.fillna(value='missing', inplace=True)\n",
    "    X_test_cat.fillna(value='missing', inplace=True)\n",
    "    # OneHotEncode Categorical variables\n",
    "    ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "    X_train_ohe = ohe.fit_transform(X_train_cat)\n",
    "    X_test_ohe = ohe.transform(X_test_cat)\n",
    "    columns = ohe.get_feature_names(input_features=X_train_cat.columns)\n",
    "    cat_train_df = pd.DataFrame(X_train_ohe.todense(), columns=columns)\n",
    "    cat_test_df = pd.DataFrame(X_test_ohe.todense(), columns=columns)\n",
    "    # Combine categorical and continuous features into the final dataframe\n",
    "    X_train_all = pd.concat([pd.DataFrame(X_train_imputed_scaled), cat_train_df], axis=1)\n",
    "    X_test_all = pd.concat([pd.DataFrame(X_test_imputed_scaled), cat_test_df], axis=1)\n",
    "    return X_train_all, X_test_all, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scikit-learn 4-step modeling pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Import the class you plan to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: \"Instantiate\" the \"estimator\"\n",
    "\n",
    "    \"Estimator\" is scikit-learn's term for model\n",
    "    \"Instantiate\" means \"make an instance of\n",
    "    \n",
    "    Name of the object does not matter\n",
    "    Can specify tuning parameters (aka \"hyperparameters\") during this step\n",
    "    All parameters not specified are set to their defaults\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1, p = 1)\n",
    "# the n_neighbors is the parameter where you specify k\n",
    "# the p is ???\n",
    "\n",
    "print(knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Fit the model with data (aka \"model training\")\n",
    "\n",
    "    Model is learning the relationship between X and y\n",
    "    Occurs in-place\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Predict the response for a new observation\n",
    "\n",
    "    New observations are called \"out-of-sample\" data\n",
    "    Uses the information it learned during the model training process\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.predict([[3, 5, 4, 2]])\n",
    "\n",
    "# Result ---> array([1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    Returns a NumPy array\n",
    "    Can predict for multiple observations at once\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = [[3, 5, 4, 2], [5, 4, 3, 2]]\n",
    "knn.predict(X_new)\n",
    "\n",
    "# Result ---> array([1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning the k parameter \n",
    "# instantiate the model (using the value K=5)\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# fit the model with data\n",
    "knn.fit(X, y)\n",
    "\n",
    "# predict the response for new observations\n",
    "knn.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import load_iris function from datasets module\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# save \"bunch\" object containing iris dataset and its attributes\n",
    "iris = load_iris()\n",
    "\n",
    "# store feature matrix in \"X\"\n",
    "X = iris.data\n",
    "\n",
    "# store response vector in \"y\"\n",
    "y = iris.target\n",
    "\n",
    "#apply the ML algorithm \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)\n",
    "scaler = StandardScaler()  \n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)  \n",
    "X_test = scaler.transform(X_test)  \n",
    "\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_predict = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "# F1 = 2 * (precision * recall) / (precision + recall)\n",
    "accuracy_score(y_test, y_predict)\n",
    "f1_score(y_test, y_predict, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, y_predict, labels=None, sample_weight=None))\n",
    "print(classification_report(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching for the Optimal Hyperparameter K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for an optimal value of K for KNN\n",
    "from sklearn.metrics import precision_score\n",
    "k_range = list(range(1, 11))\n",
    "k_scores = []\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_predict = knn.predict(X_test)\n",
    "    score = precision_score(y_test, y_predict, average = 'macro')\n",
    "    k_scores.append( score)\n",
    "print(k_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "plt.figure(figsize=(12, 6))  \n",
    "plt.plot(range(1, 11), k_scores, color='red', linestyle='dashed', marker='o',  \n",
    "         markerfacecolor='blue', markersize=10)\n",
    "plt.title('F1 score by K Value')  \n",
    "plt.xlabel('K Value')  \n",
    "plt.ylabel('accuracy Score') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descision Tree (Continuous Values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) we want to Train Test Split our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2, random_state = 42)\n",
    "\n",
    "# instantiate a regression instance\n",
    "regTree = DecisionTreeRegressor(max_depth=2)\n",
    "# fit the tree\n",
    "regTree.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) visualizing the regression tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for issues \n",
    "dot_data = StringIO()\n",
    "export_graphviz(regTree, out_file=dot_data,  \n",
    "                special_characters=True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) use the model to predict the testing and compare MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regPred = regTree.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Comparing the results and plot them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing the results and plot them \n",
    "mse = mean_squared_error(regPred, y_test)\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To avoid overfitting our data by going too deep into our tree we need to tune hyperparameter max_depth\n",
    "\n",
    "#### And prune it back to a subtree by using cross-validation to find the lowest possible MSE (validation curve)\n",
    "\n",
    "#### Therefore we want to use k-fold cross-validation to find out the optimal depth of a regression tree\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "depth_range = range(1,10)\n",
    "mse = []\n",
    "for depth in depth_range:\n",
    "    regtree = DecisionTreeRegressor(max_depth = depth)\n",
    "    depth_score = cross_val_score(regtree, X, y, scoring = 'neg_mean_squared_error',cv = 6)\n",
    "    mse.append(depth_score.mean())\n",
    "print(mse)\n",
    "mse = [abs(number) for number in mse]\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.plot(depth_range, mse)\n",
    "plt.xlabel('range of depth')\n",
    "plt.ylabel('mse')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descision Tree (Classification Values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) we want to Train Test Split our data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_iris,X_test_iris,y_train_iris,y_test_iris = train_test_split(X_iris, y_iris, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "ctree=DecisionTreeClassifier(max_depth = 3)\n",
    "ctree.fit(X_train_iris,y_train_iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) visualizing the regression tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = StringIO()\n",
    "export_graphviz(ctree, out_file=dot_data,  \n",
    "                rounded=True,\n",
    "                special_characters=True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) use the model to predict the testing and compare MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clasPred = ctree.predict(X_test_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test_iris, clasPred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The â€˜valueâ€™ row in each node tells us how many of the observations that were sorted into that node fall into each of our three categories. We can see that our feature X2, which is the petal length, was able to completely distinguish one species of flower (Iris-Setosa) from the rest.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Comparing the results and plot them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "score = cross_val_score(ctree, X_iris, y_iris, cv = 10)\n",
    "score.mean()\n",
    "depth_range = range(1,10)\n",
    "val = []\n",
    "for depth in depth_range:\n",
    "    ctree = DecisionTreeClassifier(max_depth = depth)\n",
    "    depth_score = cross_val_score(ctree, X_iris, y_iris, cv = 10)\n",
    "    val.append(depth_score.mean())\n",
    "print(val)\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.plot(depth_range, val)\n",
    "plt.xlabel('range of depth')\n",
    "plt.ylabel('cross validated values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.score()\n",
    "\n",
    "#accuracy of the log reg of the dataset on correct values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
